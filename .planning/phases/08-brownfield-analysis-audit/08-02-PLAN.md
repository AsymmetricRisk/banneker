---
phase: 08-brownfield-analysis-audit
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - templates/agents/banneker-auditor.md
  - templates/config/completeness-rubric.md
autonomous: true

must_haves:
  truths:
    - "Auditor agent reads plan files and evaluates against engineering completeness rubric"
    - "Auditor produces a scored coverage report with 10 categories"
    - "Auditor identifies specific gaps and provides actionable recommendations"
    - "Completeness rubric defines 10 categories with weighted criteria"
  artifacts:
    - path: "templates/agents/banneker-auditor.md"
      provides: "Plan evaluation sub-agent with rubric-based scoring"
      contains: "banneker-auditor"
    - path: "templates/config/completeness-rubric.md"
      provides: "Engineering completeness rubric with 10 scored categories"
      contains: "ROLES-ACTORS"
  key_links:
    - from: "templates/agents/banneker-auditor.md"
      to: "templates/config/completeness-rubric.md"
      via: "agent reads rubric to know evaluation criteria"
      pattern: "completeness-rubric"
    - from: "templates/agents/banneker-auditor.md"
      to: ".banneker/audit-report.json"
      via: "agent writes structured audit results"
      pattern: "audit-report"
---

<objective>
Create the auditor sub-agent and engineering completeness rubric for evaluating plans against a structured quality standard.

Purpose: The auditor is the core agent behind `/banneker:audit`. It reads plan documents, evaluates each against the completeness rubric's 10 categories, scores coverage, identifies gaps, and produces both JSON and Markdown reports. The rubric config file defines the evaluation criteria used by the auditor.

Output: `templates/agents/banneker-auditor.md` (agent prompt) and `templates/config/completeness-rubric.md` (rubric definition).
</objective>

<execution_context>
@/home/daniel/.claude/get-shit-done/workflows/execute-plan.md
@/home/daniel/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-brownfield-analysis-audit/08-RESEARCH.md

# Reference existing patterns
@templates/agents/banneker-exporter.md
@templates/config/document-catalog.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create completeness rubric config file</name>
  <files>templates/config/completeness-rubric.md</files>
  <action>
Create `templates/config/completeness-rubric.md` following the established config file pattern (see document-catalog.md for reference). Config files have NO YAML frontmatter - they are pure markdown.

Define the engineering completeness rubric with 10 categories. Each category has:
- **ID** (e.g., ROLES-ACTORS)
- **Name** (e.g., Roles and Actors)
- **Description** (what this category covers)
- **Weight** (importance multiplier: 1.0 for standard, 1.5 for important, 2.0 for critical)
- **Criteria** (3-5 specific, measurable evaluation criteria with search terms)

**The 10 categories (from research):**

1. **ROLES-ACTORS** (weight 1.0) - User roles, system actors, capabilities
   - All human user roles identified and defined
   - System actors (external services, APIs) documented
   - Actor capabilities and permissions specified

2. **DATA-MODEL** (weight 1.5) - Entities, attributes, relationships, constraints
   - Entities and their attributes defined
   - Relationships between entities documented
   - Constraints and validation rules specified

3. **API-SURFACE** (weight 1.5) - Endpoints, request/response formats, auth
   - API endpoints defined with HTTP methods
   - Request/response formats documented
   - Authentication requirements specified

4. **AUTH-AUTHZ** (weight 1.5) - Authentication, authorization, access control
   - Authentication mechanism chosen and documented
   - Authorization model defined
   - Credential storage and security approach specified

5. **INFRASTRUCTURE** (weight 1.5) - Hosting, deployment, scaling
   - Hosting platform chosen
   - Scalability approach documented
   - Monitoring and observability strategy defined

6. **ERROR-HANDLING** (weight 1.0) - Error detection, handling, recovery, feedback
   - Error cases identified for key operations
   - Error handling strategy documented
   - User-facing error messages and feedback defined

7. **TESTING** (weight 1.5) - Unit, integration, E2E testing, coverage
   - Unit testing framework and approach chosen
   - Integration testing strategy defined
   - E2E testing approach documented (if applicable)
   - Coverage targets and CI integration specified

8. **SECURITY** (weight 2.0) - Threats, mitigations, secure coding
   - Security threats identified (XSS, CSRF, injection, etc.)
   - Mitigation strategies documented
   - Secure coding practices specified

9. **PERFORMANCE** (weight 1.0) - Targets, optimization, bottlenecks
   - Performance targets defined (response time, throughput)
   - Optimization strategies documented
   - Potential bottlenecks identified

10. **DEPLOYMENT** (weight 1.5) - Pipeline, environments, rollback, releases
    - Deployment environments defined (dev, staging, prod)
    - CI/CD pipeline documented
    - Rollback and release process specified

For each criterion, include a "Detection guidance" line with specific search terms/patterns the auditor should look for. This makes evaluation repeatable.

**Scoring section** at the end:
- Per-category score: (met criteria / total criteria) * 100
- Weighted score: category score * weight
- Overall score: sum(weighted scores) / sum(weights)
- Grade mapping: A (90+), B (80-89), C (70-79), D (60-69), F (<60)
- Status mapping: COMPLETE (90+), MOSTLY_COMPLETE (70-89), PARTIAL (50-69), INCOMPLETE (<50)
  </action>
  <verify>
1. File exists at templates/config/completeness-rubric.md
2. File does NOT start with YAML frontmatter (no --- delimiters at top)
3. File contains all 10 category IDs: ROLES-ACTORS, DATA-MODEL, API-SURFACE, AUTH-AUTHZ, INFRASTRUCTURE, ERROR-HANDLING, TESTING, SECURITY, PERFORMANCE, DEPLOYMENT
4. Each category has weight, criteria (3-5 each), and detection guidance
5. File includes scoring formula section
  </verify>
  <done>
templates/config/completeness-rubric.md exists as a pure markdown file (no frontmatter), contains all 10 engineering completeness categories with weighted criteria and detection guidance, and includes scoring formulas.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create auditor agent prompt file</name>
  <files>templates/agents/banneker-auditor.md</files>
  <action>
Create `templates/agents/banneker-auditor.md` following the established agent file pattern.

**YAML frontmatter:**
```yaml
---
name: banneker-auditor
description: "Evaluate engineering plans against the completeness rubric and produce scored coverage reports. Reads plan files, scores 10 categories, identifies gaps, and generates actionable recommendations."
---
```

**Agent body must include these sections:**

1. **Role and Context** - You are the Banneker Auditor. You evaluate engineering plans against the completeness rubric. You are spawned by the banneker-audit command orchestrator. You produce dual output: audit-report.json (structured) and audit-report.md (human-readable).

2. **Input Loading** - Define what the auditor reads:
   - Plan files to audit (provided by command orchestrator - typically .planning/phases/ directories or any markdown plan files)
   - Completeness rubric from config/completeness-rubric.md (installed at {runtime}/config/)
   - Optionally ROADMAP.md to check for deferred items (don't penalize Phase 1 for Phase 5 topics)

3. **Evaluation Process** - 3-step evaluation:
   - Step 1: Load all plan content into a combined text corpus
   - Step 2: For each rubric category, evaluate each criterion using detection guidance (search for keywords/patterns in plan content)
   - Step 3: Score each category, compute weighted overall score

4. **Scoring Logic** (mirrors rubric config):
   - Criterion is "met" if at least 2 relevant terms/patterns found in plan content (fuzzy matching, not exact keyword)
   - Per-category: (met / total) * 100
   - Weighted: category_score * weight
   - Overall: sum(weighted) / sum(weights)
   - Grade: A/B/C/D/F based on weighted percentage
   - Status: COMPLETE/MOSTLY_COMPLETE/PARTIAL/INCOMPLETE

5. **Gap Analysis** - For each unmet criterion:
   - Identify which category it belongs to
   - Record the specific criterion description
   - Generate an actionable recommendation (what to add and where)
   - Assign priority: HIGH (INCOMPLETE categories), MEDIUM (PARTIAL categories), LOW (MOSTLY_COMPLETE categories)

6. **Output Format - JSON** (.banneker/audit-report.json):
   - audit_metadata: version, audited timestamp, plan_files list
   - overall_score: percentage, weighted_percentage, grade, status
   - category_scores: array of {category_id, category_name, score, weighted_score, weight, met_criteria, total_criteria, status, gaps}
   - recommendations: array of {priority, category, action}

7. **Output Format - Markdown** (.banneker/audit-report.md):
   - Header with audit date and overall score/grade
   - Summary table: Category | Score | Status | Gaps
   - Recommendations section (prioritized: HIGH first, then MEDIUM, then LOW)
   - Gap Details section: for each non-COMPLETE category, list missing criteria

8. **Quality Rules:**
   - Never report a criterion as met unless evidence found in plan content
   - Recommendations must be specific and actionable ("Add deployment platform choice to infrastructure section"), not vague ("Improve infrastructure")
   - Account for plan dependencies: if ROADMAP.md shows testing is Phase 2 and we're auditing Phase 1, don't penalize for missing testing
   - Report the number of plan files read and total content size

9. **Completion Protocol:**
   - Write both .banneker/audit-report.json and .banneker/audit-report.md
   - Report: overall grade, number of COMPLETE categories, number of gaps found
   - If all categories COMPLETE: "Plans fully cover engineering completeness rubric"
   - If gaps exist: "N gaps found across M categories. See audit-report.md for details."

Do NOT include executable JavaScript. This is a pure markdown prompt file.
  </action>
  <verify>
1. File exists at templates/agents/banneker-auditor.md
2. File starts with valid YAML frontmatter (name: banneker-auditor)
3. File contains sections for: Role, Input Loading, Evaluation Process, Scoring Logic, Gap Analysis, JSON Output, Markdown Output, Quality Rules, Completion Protocol
4. File references completeness-rubric.md config
5. File defines both .banneker/audit-report.json and .banneker/audit-report.md output paths
6. File does NOT contain executable JavaScript
  </verify>
  <done>
templates/agents/banneker-auditor.md exists with YAML frontmatter (name: banneker-auditor), contains all 9 required sections, references the completeness rubric config, defines dual output formats (JSON + Markdown), and follows the established agent file pattern.
  </done>
</task>

</tasks>

<verification>
- `head -5 templates/config/completeness-rubric.md` shows pure markdown (no frontmatter)
- `head -5 templates/agents/banneker-auditor.md` shows valid YAML frontmatter
- Rubric contains all 10 categories with weights and criteria
- Auditor agent references rubric and defines both output formats
- Both files follow established patterns from prior phases
</verification>

<success_criteria>
- Completeness rubric config exists with 10 weighted categories and 30+ criteria
- Auditor agent file exists with valid frontmatter and complete evaluation instructions
- Auditor references the rubric config for evaluation criteria
- Dual output format (JSON + Markdown) fully specified
- Gap analysis produces actionable, prioritized recommendations
</success_criteria>

<output>
After completion, create `.planning/phases/08-brownfield-analysis-audit/08-02-SUMMARY.md`
</output>
